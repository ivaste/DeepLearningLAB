{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gRboflunHqye"
   },
   "source": [
    "#NEURAL NETWORKS AND DEEP LEARNING\n",
    "> M.Sc. ICT FOR LIFE AND HEALTH\n",
    "> \n",
    "> Department of Information Engineering\n",
    "\n",
    "> M.Sc. COMPUTER ENGINEERING\n",
    ">\n",
    "> Department of Information Engineering\n",
    "\n",
    "> M.Sc. AUTOMATION ENGINEERING\n",
    ">\n",
    "> Department of Information Engineering\n",
    " \n",
    "> M.Sc. PHYSICS OF DATA\n",
    ">\n",
    "> Department of Physics and Astronomy\n",
    " \n",
    "> M.Sc. COGNITIVE NEUROSCIENCE AND CLINICAL NEUROPSYCHOLOGY\n",
    ">\n",
    "> Department of General Psychology\n",
    "\n",
    "---\n",
    "A.A. 2020/21 (6 CFU) - Dr. Alberto Testolin, Dr. Matteo Gadaleta\n",
    "---\n",
    "\n",
    "\n",
    "##Lab. 03 - PyTorch regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ODcSwFcDXrtD"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XBMlgnam0Yz0"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p82k5GjECklA"
   },
   "source": [
    "# Practical example: Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7QLy_wTqDVM1"
   },
   "source": [
    "Now let's solve the same problem analyzed in one of the previous lab using a simple neural network implemented in PyTorch.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wf9Zr5WjUZnB"
   },
   "source": [
    "# Data generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5h0IGLgfUha0"
   },
   "source": [
    "Let's generate some data with our usual polynomial model, and save the data points in two csv files, one for training (train_data.csv), and one for validation (val_data.csv).\n",
    "\n",
    "You can find these files in the \"Files\" section of Colab. They are not stored in your local machine, but they are stored remotely in the Colab server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VjGBsKhQUgwl"
   },
   "outputs": [],
   "source": [
    "def poly_model(x, beta, noise_std=0):\n",
    "    \"\"\"\n",
    "    INPUT\n",
    "        x: x vector\n",
    "        beta: polynomial parameters\n",
    "        noise_std: enable noisy sampling (gaussian noise, zero mean, noise_std std)\n",
    "    \"\"\"\n",
    "    pol_order = len(beta)\n",
    "    x_matrix = np.array([x**i for i in range(pol_order)]).transpose()\n",
    "    y = np.matmul(x_matrix, beta)\n",
    "    noise = np.random.randn(len(y)) * noise_std\n",
    "    return y + noise\n",
    "\n",
    "beta_true = [-1.45, 1.12, 2.3]\n",
    "noise_std = 0.2\n",
    "np.random.seed(4)\n",
    "\n",
    "### Train data\n",
    "num_train_points = 20\n",
    "x_train = np.random.rand(num_train_points)\n",
    "y_train = poly_model(x_train, beta_true, noise_std)\n",
    "with open('train_data.csv', 'w') as f:\n",
    "  data = [f\"{x},{y}\" for x, y in zip(x_train, y_train)]\n",
    "  f.write('\\n'.join(data))\n",
    "    \n",
    "### Validation data\n",
    "num_val_points = 20\n",
    "x_val = np.random.rand(num_val_points)\n",
    "y_val = poly_model(x_val, beta_true, noise_std)\n",
    "with open('val_data.csv', 'w') as f:\n",
    "  data = [f\"{x},{y}\" for x, y in zip(x_val, y_val)]\n",
    "  f.write('\\n'.join(data))\n",
    "\n",
    "  \n",
    "### Plot\n",
    "plt.figure(figsize=(12,8))\n",
    "x_highres = np.linspace(0,1,1000)\n",
    "plt.plot(x_highres, poly_model(x_highres, beta_true), color='b', ls='--', label='True data model')\n",
    "plt.plot(x_train, y_train, color='r', ls='', marker='.', label='Train data points')\n",
    "plt.plot(x_val, y_val, color='g', ls='', marker='.', label='Validation data points')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yg5hsr4Vk92E"
   },
   "source": [
    "# Network Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zusKyw-4EUN7"
   },
   "source": [
    "## Network Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FV6KyWPAEYxL"
   },
   "source": [
    "Define a fully connected feed-forward network with 2 hidden layers.\n",
    "\n",
    "Use a sigmoid activation function.\n",
    "\n",
    "Since this is a regression, we do not want to limit the value of the output. For this reason, NO activation function should be used for the output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iCFiQ0t5EsnM"
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    \n",
    "    def __init__(self, Ni, Nh1, Nh2, No):\n",
    "        \"\"\"\n",
    "        Ni - Input size\n",
    "        Nh1 - Neurons in the 1st hidden layer\n",
    "        Nh2 - Neurons in the 2nd hidden layer\n",
    "        No - Output size\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        print('Network initialized')\n",
    "        self.fc1 = nn.Linear(in_features=Ni, out_features=Nh1)\n",
    "        self.fc2 = nn.Linear(in_features=Nh1, out_features=Nh2)\n",
    "        self.out = nn.Linear(in_features=Nh2, out_features=No)\n",
    "        self.act = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x, additional_out=False):\n",
    "        x = self.act(self.fc1(x))\n",
    "        x = self.act(self.fc2(x))\n",
    "        x = self.out(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "loocdtx_EZWw"
   },
   "source": [
    "## Dataset and Dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kG-twDQsETb1"
   },
   "source": [
    "\n",
    "We have already implemented the dataset class in the previous lab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ilKnlHjBgG81"
   },
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Lleqk78jfyBc"
   },
   "outputs": [],
   "source": [
    "class CsvDataset(Dataset):\n",
    "\n",
    "  def __init__(self, csv_file, transform=None):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        csv_file (string): Path to the csv file.\n",
    "        transform (callable, optional): Optional transform to be applied\n",
    "            on a sample.\n",
    "    \"\"\"\n",
    "    self.transform = transform\n",
    "    # Read the file and split the lines in a list\n",
    "    with open(csv_file, 'r') as f:\n",
    "      lines = f.read().split('\\n')\n",
    "    # Get x and y values from each line and append to self.data\n",
    "    self.data = []\n",
    "    for line in lines:\n",
    "      sample = line.split(',')\n",
    "      self.data.append((float(sample[0]), float(sample[1])))\n",
    "    # Now self.data contains all our dataset.\n",
    "    # Each element of the list self.data is a tuple: (input, output)\n",
    "\n",
    "  def __len__(self):\n",
    "    # The length of the dataset is simply the length of the self.data list\n",
    "    return len(self.data)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    # Our sample is the element idx of the list self.data\n",
    "    sample = self.data[idx]\n",
    "    if self.transform:\n",
    "        sample = self.transform(sample)\n",
    "    return sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5qYRE5c4gJoe"
   },
   "source": [
    "### Transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dcevK-ilf3iv"
   },
   "outputs": [],
   "source": [
    "class ToTensor(object):\n",
    "    \"\"\"Convert sample to Tensors.\"\"\"\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        x, y = sample\n",
    "        return (torch.tensor([x]).float(),\n",
    "                torch.tensor([y]).float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Yf8hVkF_D141"
   },
   "outputs": [],
   "source": [
    "composed_transform = transforms.Compose([ToTensor()])\n",
    "\n",
    "train_dataset = CsvDataset('train_data.csv', transform=composed_transform)\n",
    "val_dataset = CsvDataset('val_data.csv', transform=composed_transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3nXQIlz3gPeP"
   },
   "source": [
    "### Dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7lvOg3jhFIe5"
   },
   "source": [
    "For the dataloader:\n",
    "\n",
    "* enable the shuffling only for training data\n",
    "* try different values for batch size\n",
    "* disable the multiprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f1Y1XPYQFQLk"
   },
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=4, shuffle=True, num_workers=0)\n",
    "val_dataloader  = DataLoader(val_dataset,  batch_size=len(val_dataset), shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hWVcMe9jFCpv"
   },
   "source": [
    "## Training loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yK8IMi5mGAxy"
   },
   "source": [
    "Now we put together all the steps analyzed in the previous lab:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U24WThWLHcEC"
   },
   "outputs": [],
   "source": [
    "# Check if the GPU is available\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(f\"Training device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gn7W-MAmgxai"
   },
   "outputs": [],
   "source": [
    "# Initialize the network\n",
    "torch.manual_seed(0)\n",
    "Ni = 1\n",
    "Nh1 = 128\n",
    "Nh2 = 256\n",
    "No = 1\n",
    "net = Net(Ni, Nh1, Nh2, No)\n",
    "net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XDt5U_Wgg1Mh"
   },
   "outputs": [],
   "source": [
    "# Define the loss function\n",
    "loss_fn = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HgoFcUfPg116"
   },
   "outputs": [],
   "source": [
    "# Define the optimizer\n",
    "optimizer = optim.Adam(net.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hLPzqHepg2RZ"
   },
   "outputs": [],
   "source": [
    "### TRAINING LOOP\n",
    "num_epochs = 3000\n",
    "train_loss_log = []\n",
    "val_loss_log = []\n",
    "for epoch_num in range(num_epochs):\n",
    "  print('#################')\n",
    "  print(f'# EPOCH {epoch_num}')\n",
    "  print('#################')\n",
    "\n",
    "  ### TRAIN\n",
    "  train_loss= []\n",
    "  net.train() # Training mode (e.g. enable dropout)\n",
    "  for sample_batched in train_dataloader:\n",
    "    # Move data to device\n",
    "    x_batch = sample_batched[0].to(device)\n",
    "    label_batch = sample_batched[1].to(device)\n",
    "\n",
    "    # Forward pass\n",
    "    out = net(x_batch)\n",
    "\n",
    "    # Compute loss\n",
    "    loss = loss_fn(out, label_batch)\n",
    "\n",
    "    # Backpropagation\n",
    "    net.zero_grad()\n",
    "    loss.backward()\n",
    "\n",
    "    # Update the weights\n",
    "    optimizer.step()\n",
    "\n",
    "    # Save train loss for this batch\n",
    "    loss_batch = loss.detach().cpu().numpy()\n",
    "    train_loss.append(loss_batch)\n",
    "\n",
    "  # Save average train loss\n",
    "  train_loss = np.mean(train_loss)\n",
    "  print(f\"AVERAGE TRAIN LOSS: {train_loss}\")\n",
    "  train_loss_log.append(train_loss)\n",
    "\n",
    "  ### VALIDATION\n",
    "  val_loss= []\n",
    "  net.eval() # Evaluation mode (e.g. disable dropout)\n",
    "  with torch.no_grad(): # Disable gradient tracking\n",
    "    for sample_batched in val_dataloader:\n",
    "      # Move data to device\n",
    "      x_batch = sample_batched[0].to(device)\n",
    "      label_batch = sample_batched[1].to(device)\n",
    "\n",
    "      # Forward pass\n",
    "      out = net(x_batch)\n",
    "\n",
    "      # Compute loss\n",
    "      loss = loss_fn(out, label_batch)\n",
    "\n",
    "      # Save val loss for this batch\n",
    "      loss_batch = loss.detach().cpu().numpy()\n",
    "      val_loss.append(loss_batch)\n",
    "\n",
    "    # Save average validation loss\n",
    "    val_loss = np.mean(val_loss)\n",
    "    print(f\"AVERAGE VAL LOSS: {np.mean(val_loss)}\")\n",
    "    val_loss_log.append(val_loss)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XS0i-CXukrPX"
   },
   "source": [
    "### Plot losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FOY4yWW8lqwb"
   },
   "outputs": [],
   "source": [
    "# Plot losses\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.semilogy(train_loss_log, label='Train loss')\n",
    "plt.semilogy(val_loss_log, label='Validation loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XV0Kingak37m"
   },
   "source": [
    "# Network analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PMe88uzesM2s"
   },
   "source": [
    "## Network output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xW2JB_NqmhSo"
   },
   "outputs": [],
   "source": [
    "# Input vector\n",
    "x_vec = torch.linspace(0,1,1000)\n",
    "x_vec = x_vec.to(device)\n",
    "x_vec = x_vec.unsqueeze(-1)\n",
    "print(f\"Input shape: {x_vec.shape}\")\n",
    "\n",
    "# Network output\n",
    "with torch.no_grad():\n",
    "  y_vec = net(x_vec)\n",
    "print(f\"Output shape: {y_vec.shape}\")\n",
    "\n",
    "# Expected output\n",
    "beta_true = [-1.45, 1.12, 2.3]\n",
    "true_model = poly_model(x_vec.cpu().numpy(), beta_true).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mQ0atER6pl3S"
   },
   "outputs": [],
   "source": [
    "# Convert x_vec and y_vec to numpy one dimensional arrays\n",
    "x_vec = x_vec.squeeze().cpu().numpy()\n",
    "y_vec = y_vec.squeeze().cpu().numpy()\n",
    "\n",
    "# Plot output\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.plot(x_vec, y_vec, label='Network output')\n",
    "plt.plot(x_vec, true_model, label='True model')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5w7mO62wLQUG"
   },
   "source": [
    "## Access network parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uDgv9tIJkvKn"
   },
   "outputs": [],
   "source": [
    "# First hidden layer\n",
    "h1_w = net.fc1.weight.data.cpu().numpy()\n",
    "h1_b = net.fc1.bias.data.cpu().numpy()\n",
    "\n",
    "# Second hidden layer\n",
    "h2_w = net.fc2.weight.data.cpu().numpy()\n",
    "h2_b = net.fc2.bias.data.cpu().numpy()\n",
    "\n",
    "# Output layer\n",
    "out_w = net.out.weight.data.cpu().numpy()\n",
    "out_b = net.out.bias.data.cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xl66-WTllFlK"
   },
   "source": [
    "## Weights histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zNS27jm0kSVH"
   },
   "outputs": [],
   "source": [
    "# Weights histogram\n",
    "fig, axs = plt.subplots(3, 1, figsize=(12,8))\n",
    "axs[0].hist(h1_w.flatten(), 50)\n",
    "axs[0].set_title('First hidden layer weights')\n",
    "axs[1].hist(h2_w.flatten(), 50)\n",
    "axs[1].set_title('Second hidden layer weights')\n",
    "axs[2].hist(out_w.flatten(), 50)\n",
    "axs[2].set_title('Output layer weights')\n",
    "[ax.grid() for ax in axs]\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M8Xo32EvlJMt"
   },
   "source": [
    "## Save network parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T3hQi0qMlRfI"
   },
   "source": [
    "### Save network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q45V3z1nkTQ5"
   },
   "outputs": [],
   "source": [
    "### Save network parameters\n",
    "### Save the network state\n",
    "# The state dictionary includes all the parameters of the network\n",
    "net_state_dict = net.state_dict()\n",
    "# Save the state dict to a file\n",
    "torch.save(net_state_dict, 'net_parameters.torch')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N9ez-ztylTWW"
   },
   "source": [
    "### Load network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4u834VExlOe5"
   },
   "outputs": [],
   "source": [
    "### Reload the network state\n",
    "# First initialize the network (if not already done)\n",
    "net = Net(Ni, Nh1, Nh2, No) \n",
    "# Load the state dict previously saved\n",
    "net_state_dict = torch.load('net_parameters.torch')\n",
    "# Update the network parameters\n",
    "net.load_state_dict(net_state_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rlzmh2vZnJHz"
   },
   "source": [
    "## Save optimizer state\n",
    "Also the optimizer has its internal state!\n",
    "\n",
    "You need to save both the network and the optimizer states if you want to continue your training.\n",
    "\n",
    "If you are sure you have finished your training you can just save the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g2kgoLEtnC2q"
   },
   "outputs": [],
   "source": [
    "### Save the optimizer state\n",
    "torch.save(optimizer.state_dict(), 'optimizer_state.torch')\n",
    "\n",
    "### Reload the optimizer state\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
    "opt_state_dict = torch.load('optimizer_state.torch')\n",
    "optimizer.load_state_dict(opt_state_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WgeV0XEmm7yJ"
   },
   "source": [
    "## Analyze activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IXdvC7e00z7_"
   },
   "outputs": [],
   "source": [
    "def get_activation(layer, input, output):\n",
    "    global activation\n",
    "    activation = torch.sigmoid(output)\n",
    "\n",
    "### Register hook  \n",
    "hook_handle = net.fc2.register_forward_hook(get_activation)\n",
    "\n",
    "### Analyze activations\n",
    "net = net.to(device)\n",
    "net.eval()\n",
    "with torch.no_grad():\n",
    "    x1 = torch.tensor([0.1]).float().to(device)\n",
    "    y1 = net(x1)\n",
    "    z1 = activation\n",
    "    x2 = torch.tensor([0.9]).float().to(device)\n",
    "    y2 = net(x2)\n",
    "    z2 = activation\n",
    "    x3 = torch.tensor([2.5]).float().to(device)\n",
    "    y3 = net(x3)\n",
    "    z3 = activation\n",
    "\n",
    "### Remove hook\n",
    "hook_handle.remove()\n",
    "\n",
    "### Plot activations\n",
    "fig, axs = plt.subplots(3, 1, figsize=(12,6))\n",
    "axs[0].stem(z1.cpu().numpy(), use_line_collection=True)\n",
    "axs[0].set_title('Last layer activations for input x=%.2f' % x1)\n",
    "axs[1].stem(z2.cpu().numpy(), use_line_collection=True)\n",
    "axs[1].set_title('Last layer activations for input x=%.2f' % x2)\n",
    "axs[2].stem(z3.cpu().numpy(), use_line_collection=True)\n",
    "axs[2].set_title('Last layer activations for input x=%.2f' % x3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yeBXPMglHLyP"
   },
   "source": [
    "# Exercise - Classification model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2Eo79cojLFvC"
   },
   "source": [
    "**HINTS**\n",
    "- Choose a loss function that is suitable for the specific problem, a binary classification in this case. If you keep a single linear output you can use a BCEWithLogitsLoss, which is more numerically stable than manually using a sigmoid output activation (more info here: https://pytorch.org/docs/stable/generated/torch.nn.BCEWithLogitsLoss.html).\n",
    "- The network now has 2 inputs. A batched input should have a shape $\\text{batch_size} \\times 2$.\n",
    "- The dataset should be adapted accordingly. Also consider to increase the batch size.\n",
    "- Explore different optimizers, trying to understand the differences and their parameters (https://pytorch.org/docs/stable/optim.html).\n",
    "- Try to increase the complexity of the network, and at the same time to introduce some regularization with dropout layers and/or weight decay (which is equivalent to an L2 regularization, typically implemented by the optimizer).\n",
    "- Experiment with different hyper-parameters trying to minimize the VALIDATION LOSS. Once you are happy with the result, try the final test with the TEST dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bNn5DYNZGNz4"
   },
   "source": [
    "## Data generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OpICn2zOD4my"
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "np.random.seed(123)\n",
    "\n",
    "def bidimensional_model(x1, x2):\n",
    "  a = 1\n",
    "  b = 1\n",
    "  cx1 = 0\n",
    "  cx2 = 0\n",
    "  out = (x1 - cx1)**2 / a**2 + (x2 - cx2)**2 / b **2 < 1\n",
    "  a = 2\n",
    "  b = 2\n",
    "  cx1 = 5\n",
    "  cx2 = 5\n",
    "  out |= (x1 - cx1)**2 / a**2 + (x2 - cx2)**2 / b **2 < 1\n",
    "  a = 1\n",
    "  b = 2\n",
    "  cx1 = -2.5\n",
    "  cx2 = 5\n",
    "  out |= (x1 - cx1)**2 / a**2 + (x2 - cx2)**2 / b **2 < 1\n",
    "  a = 3\n",
    "  b = 1\n",
    "  cx1 = -6\n",
    "  cx2 = -2.5\n",
    "  out |= (x1 - cx1)**2 / a**2 + (x2 - cx2)**2 / b **2 < 1\n",
    "  a = 2\n",
    "  b = 4\n",
    "  cx1 = -7.5\n",
    "  cx2 = -5\n",
    "  out |= (x1 - cx1)**2 / a**2 + (x2 - cx2)**2 / b **2 < 1\n",
    "  a = 6\n",
    "  b = 1\n",
    "  cx1 = -7.5\n",
    "  cx2 = 5\n",
    "  out |= (x1 - cx1)**2 / a**2 + (x2 - cx2)**2 / b **2 < 1\n",
    "  a = 4\n",
    "  b = 4\n",
    "  cx1 = 7.5\n",
    "  cx2 = -7.5\n",
    "  out |= (x1 - cx1)**2 / a**2 + (x2 - cx2)**2 / b **2 < 1\n",
    "  a = 3\n",
    "  b = 2\n",
    "  cx1 = -1\n",
    "  cx2 = -6\n",
    "  out |= (x1 - cx1)**2 / a**2 + (x2 - cx2)**2 / b **2 < 1\n",
    "  a = 2\n",
    "  b = 5\n",
    "  cx1 = 1\n",
    "  cx2 = 6\n",
    "  out |= (x1 - cx1)**2 / a**2 + (x2 - cx2)**2 / b **2 < 1\n",
    "  a = 2\n",
    "  b = 2\n",
    "  cx1 = 6\n",
    "  cx2 = 0\n",
    "  out |= (x1 - cx1)**2 / a**2 + (x2 - cx2)**2 / b **2 < 1\n",
    "  return out.astype(int)\n",
    "\n",
    "### PLOT MODEL\n",
    "# Input grid\n",
    "x1 = np.linspace(-10, 10, 400)\n",
    "x2 = np.linspace(-10, 10, 400)\n",
    "x_prod = [x for x in itertools.product(x1, x2)]\n",
    "x1 = np.array([x[0] for x in x_prod])\n",
    "x2 = np.array([x[1] for x in x_prod])\n",
    "# Evaluate out\n",
    "y = bidimensional_model(x1, x2)\n",
    "# Scatter plot\n",
    "fig, ax = plt.subplots(figsize=(12,8))\n",
    "ax.scatter(x1, x2, c=y, s=1, marker='o')\n",
    "ax.set_xlabel('x1')\n",
    "ax.set_ylabel('x2')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zYWWVtCyGT2k"
   },
   "source": [
    "### Training points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J-Jp4QJc95dq"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "### Train data\n",
    "num_points = 1000\n",
    "x1 = np.random.uniform(-10, 10, num_points)\n",
    "x2 = np.random.uniform(-10, 10, num_points)\n",
    "y = bidimensional_model(x1, x2)\n",
    "train_df = pd.DataFrame({'x1': x1, 'x2': x2, 'y': y})\n",
    "train_df.to_csv('classifier_train_data.csv', index=False)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12,8))\n",
    "ax.scatter(x1, x2, c=y, s=1, marker='o')\n",
    "ax.set_xlabel('x1')\n",
    "ax.set_ylabel('x2')\n",
    "ax.set_title('Training points')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SitIaovOGVvi"
   },
   "source": [
    "### Validation points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FjRdHgXXFWic"
   },
   "source": [
    "Validation points are randomly selected from the training points (20% in this case)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Nu1GcLMmJ7cS"
   },
   "outputs": [],
   "source": [
    "### Validation data\n",
    "num_points = 200\n",
    "x1 = np.random.uniform(-10, 10, num_points)\n",
    "x2 = np.random.uniform(-10, 10, num_points)\n",
    "y = bidimensional_model(x1, x2)\n",
    "val_df = pd.DataFrame({'x1': x1, 'x2': x2, 'y': y})\n",
    "val_df.to_csv('classifier_val_data.csv', index=False)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12,8))\n",
    "ax.scatter(x1, x2, c=y, s=1, marker='o')\n",
    "ax.set_xlabel('x1')\n",
    "ax.set_ylabel('x2')\n",
    "ax.set_title('Validation points')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5k2jww94GXhM"
   },
   "source": [
    "### Test points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ClhCIOQmKoIq"
   },
   "outputs": [],
   "source": [
    "### Test data\n",
    "num_points = 400\n",
    "x1 = np.random.uniform(-10, 10, num_points)\n",
    "x2 = np.random.uniform(-10, 10, num_points)\n",
    "y = bidimensional_model(x1, x2)\n",
    "val_df = pd.DataFrame({'x1': x1, 'x2': x2, 'y': y})\n",
    "val_df.to_csv('classifier_test_data.csv', index=False)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12,8))\n",
    "ax.scatter(x1, x2, c=y, s=1, marker='o')\n",
    "ax.set_xlabel('x1')\n",
    "ax.set_ylabel('x2')\n",
    "ax.set_title('Test points')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BAxFx__EGgWf"
   },
   "source": [
    "## Dataset and Dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FiCbilBwHNaW"
   },
   "source": [
    "Define the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EUM50Gf4MCwZ"
   },
   "outputs": [],
   "source": [
    "class ClassifierDataset(Dataset):\n",
    "\n",
    "  def __init__(self, csv_file, transform=None):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        csv_file (string): Path to the csv file.\n",
    "        transform (callable, optional): Optional transform to be applied\n",
    "            on a sample.\n",
    "    \"\"\"\n",
    "    self.transform = transform\n",
    "    # Read the file and store the content in a pandas DataFrame\n",
    "    self.df = pd.read_csv(csv_file)\n",
    "\n",
    "  def __len__(self):\n",
    "    # The length of the dataset is simply the length of the self.data list\n",
    "    return len(self.df)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    # Our sample is the row at index idx of the dataframe\n",
    "    row = self.df.iloc[idx]\n",
    "    # There are 2 inputs this time\n",
    "    sample = ([row.x1, row.x2], row.y)\n",
    "    if self.transform:\n",
    "        sample = self.transform(sample)\n",
    "    return sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5XUmOmF5HPxD"
   },
   "source": [
    "Define the transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ekBZbbBvHJq4"
   },
   "outputs": [],
   "source": [
    "class ToTensor(object):\n",
    "    \"\"\"Convert sample to Tensors.\"\"\"\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        x, y = sample\n",
    "        return (torch.Tensor(x).float(),\n",
    "                torch.Tensor([y]).float())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9NR9xVDbnQRk"
   },
   "source": [
    "Initialize the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Mg443m60HMAZ"
   },
   "outputs": [],
   "source": [
    "composed_transform = transforms.Compose([ToTensor()])\n",
    "\n",
    "train_dataset = ClassifierDataset('classifier_train_data.csv', transform=composed_transform)\n",
    "val_dataset   = ClassifierDataset('classifier_val_data.csv', transform=composed_transform)\n",
    "test_dataset  = ClassifierDataset('classifier_test_data.csv', transform=composed_transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "43MlfiTOHTL0"
   },
   "source": [
    "Define the dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l5n2vVDgJfnM"
   },
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=200, shuffle=True, num_workers=0)\n",
    "val_dataloader   = DataLoader(val_dataset,   batch_size=len(val_dataset), shuffle=False, num_workers=0)\n",
    "test_dataloader  = DataLoader(test_dataset,  batch_size=len(test_dataset), shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o4rTz2L4G6Rf"
   },
   "source": [
    "## Network definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZxrTC-LPG8XJ"
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    \n",
    "    def __init__(self, Ni, Nh1, Nh2, No):\n",
    "        \"\"\"\n",
    "        Ni - Input size\n",
    "        Nh1 - Neurons in the 1st hidden layer\n",
    "        Nh2 - Neurons in the 2nd hidden layer\n",
    "        No - Output size\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        print('Network initialized')\n",
    "        self.fc1 = nn.Linear(in_features=Ni, out_features=Nh1)\n",
    "        self.fc2 = nn.Linear(in_features=Nh1, out_features=Nh2)\n",
    "        self.out = nn.Linear(in_features=Nh2, out_features=No)\n",
    "        self.act = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x, additional_out=False):\n",
    "        x = self.act(self.fc1(x))\n",
    "        x = self.act(self.fc2(x))\n",
    "        x = self.out(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yha8MKgSG1f-"
   },
   "source": [
    "## Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E1AKIP44G3JR"
   },
   "outputs": [],
   "source": [
    "# Initialize the network\n",
    "torch.manual_seed(0)\n",
    "Ni = 2\n",
    "Nh1 = 128\n",
    "Nh2 = 256\n",
    "No = 1\n",
    "net = Net(Ni, Nh1, Nh2, No)\n",
    "net.to(device)\n",
    "\n",
    "# Define the loss function\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = optim.Adam(net.parameters(), lr=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qmgre8qHKP8R"
   },
   "outputs": [],
   "source": [
    "### TRAINING LOOP\n",
    "num_epochs = 600\n",
    "train_loss_log = []\n",
    "val_loss_log = []\n",
    "for epoch_num in range(num_epochs):\n",
    "  print('#################')\n",
    "  print(f'# EPOCH {epoch_num}')\n",
    "  print('#################')\n",
    "\n",
    "  ### TRAIN\n",
    "  train_loss= []\n",
    "  net.train() # Training mode (e.g. enable dropout)\n",
    "  for sample_batched in train_dataloader:\n",
    "    # Move data to device\n",
    "    x_batch = sample_batched[0].to(device)\n",
    "    label_batch = sample_batched[1].to(device)\n",
    "\n",
    "    # Forward pass\n",
    "    out = net(x_batch)\n",
    "\n",
    "    # Compute loss\n",
    "    loss = loss_fn(out, label_batch)\n",
    "\n",
    "    # Backpropagation\n",
    "    net.zero_grad()\n",
    "    loss.backward()\n",
    "\n",
    "    # Update the weights\n",
    "    optimizer.step()\n",
    "\n",
    "    # Save train loss for this batch\n",
    "    loss_batch = loss.detach().cpu().numpy()\n",
    "    train_loss.append(loss_batch)\n",
    "\n",
    "  # Save average train loss\n",
    "  train_loss = np.mean(train_loss)\n",
    "  print(f\"AVERAGE TRAIN LOSS: {train_loss}\")\n",
    "  train_loss_log.append(train_loss)\n",
    "\n",
    "  ### VALIDATION\n",
    "  val_loss= []\n",
    "  net.eval() # Evaluation mode (e.g. disable dropout)\n",
    "  with torch.no_grad(): # Disable gradient tracking\n",
    "    for sample_batched in val_dataloader:\n",
    "      # Move data to device\n",
    "      x_batch = sample_batched[0].to(device)\n",
    "      label_batch = sample_batched[1].to(device)\n",
    "\n",
    "      # Forward pass\n",
    "      out = net(x_batch)\n",
    "\n",
    "      # Compute loss\n",
    "      loss = loss_fn(out, label_batch)\n",
    "\n",
    "      # Save val loss for this batch\n",
    "      loss_batch = loss.detach().cpu().numpy()\n",
    "      val_loss.append(loss_batch)\n",
    "\n",
    "    # Save average validation loss\n",
    "    val_loss = np.mean(val_loss)\n",
    "    print(f\"AVERAGE VAL LOSS: {np.mean(val_loss)}\")\n",
    "    val_loss_log.append(val_loss)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VZISd_5XK5HS"
   },
   "outputs": [],
   "source": [
    "# Plot losses\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.semilogy(train_loss_log, label='Train loss')\n",
    "plt.semilogy(val_loss_log, label='Validation loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tuSQe7WCP_Cz"
   },
   "source": [
    "## Final test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8BkLq7wDnurL"
   },
   "source": [
    "Iterate the dataloader a single time and save all the outputs (in case you have multiple batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1Bksmig_P768"
   },
   "outputs": [],
   "source": [
    "all_inputs = []\n",
    "all_outputs = []\n",
    "all_labels = []\n",
    "net.eval() # Evaluation mode (e.g. disable dropout)\n",
    "with torch.no_grad(): # Disable gradient tracking\n",
    "  for sample_batched in test_dataloader:\n",
    "    # Move data to device\n",
    "    x_batch = sample_batched[0].to(device)\n",
    "    label_batch = sample_batched[1].to(device)\n",
    "    # Forward pass\n",
    "    out = net(x_batch)\n",
    "    # Save outputs and labels\n",
    "    all_inputs.append(x_batch)\n",
    "    all_outputs.append(out)\n",
    "    all_labels.append(label_batch)\n",
    "# Concatenate all the outputs and labels ina single tensor\n",
    "all_inputs  = torch.cat(all_inputs)\n",
    "all_outputs = torch.cat(all_outputs)\n",
    "all_labels  = torch.cat(all_labels)\n",
    "\n",
    "test_loss = loss_fn(all_outputs, all_labels)\n",
    "print(f\"AVERAGE TEST LOSS: {test_loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kC-ZPwuqUZEh"
   },
   "source": [
    "In this case the network has a linear output (for a better stability of the loss function). \n",
    "To have probability estimates you can apply a sigmoid to the network output.\n",
    "\n",
    "Since we just need the most probable class and we have a single output, we can consider the sign of the linear output. Negative output means that the class 0 is the most probable (probability < 50%), otherwise class 1 (probability > 50%).\n",
    "\n",
    "Essentially this network estimates the probability of the input sample to be of class 1.\n",
    "\n",
    "> **NOTE**\n",
    "> \n",
    "> You can (and should, for practice) redefine the problem by defining a network with more than one output, each of them corresponding to a specific class (2 in this case). Since the two classes are mutually exclusive, the loss function should be a CrossEntropyLoss (softmax activation). In a multi-class scenario, a BCE loss is suitable when the classes are NOT mutually exclusive.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YcAovj6bnpHy"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "Get the most probable class inferred by the network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l4-s4wdVSmrR"
   },
   "outputs": [],
   "source": [
    "# Get the most probable class inferred by the network\n",
    "all_output_classes = torch.zeros(all_outputs.shape).to(device)\n",
    "all_output_classes[all_outputs > 0] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0MKpD6NCnX4E"
   },
   "source": [
    "Evaluate the test accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qNk9_PjUSB2y"
   },
   "outputs": [],
   "source": [
    "tot_correct_out = (all_output_classes == all_labels).sum()\n",
    "test_accuracy = 100 * tot_correct_out / len(all_labels)\n",
    "print(f\"TEST ACCURACY: {test_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a55ciPBknq9P"
   },
   "source": [
    "Plot the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8pI5we58asHV"
   },
   "outputs": [],
   "source": [
    "### Plot\n",
    "x1 = all_inputs.squeeze().cpu().numpy()[:, 0]\n",
    "x2 = all_inputs.squeeze().cpu().numpy()[:, 1]\n",
    "y_true = all_labels.squeeze().cpu().numpy()\n",
    "y_pred = all_output_classes.squeeze().cpu().numpy()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12,8))\n",
    "# Plot predictions\n",
    "ax.scatter(x1, x2, c=y_pred, s=1, marker='o')\n",
    "ax.set_xlabel('x1')\n",
    "ax.set_ylabel('x2')\n",
    "ax.set_title('Network Predictions')\n",
    "# Mark wrong outputs\n",
    "error_mask = y_pred != y_true\n",
    "ax.scatter(x1[error_mask], x2[error_mask], color='red', s=30, marker='x', label='MISCLASSIFIED SAMPLES')\n",
    "plt.legend()\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyN6WNbaWFAqxmGRLe/SSvct",
   "collapsed_sections": [],
   "name": "nndl_2020_lab_03_regression_with_solutions.ipynb",
   "provenance": [
    {
     "file_id": "1w5PwADM98HzeKtV6-LZitxLkktJBx2V5",
     "timestamp": 1602430537656
    }
   ],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
