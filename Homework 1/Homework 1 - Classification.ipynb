{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((52500, 1, 28, 28), (52500,), (17500, 1, 28, 28), (17500,))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "mnist = fetch_openml('mnist_784', cache=True)\n",
    "\n",
    "X = mnist.data.astype('float32')\n",
    "y = mnist.target.astype('int64')\n",
    "\n",
    "#Nomralization of the value between 0 and 1\n",
    "X /= 255.0\n",
    "\n",
    "#Dataset for Cnn model\n",
    "X = X.reshape(-1, 1, 28, 28) #Comment if you want to use the FC model\n",
    "\n",
    "#Train - test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "X_train.shape, y_train.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1) FC3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FULLY CONNECTED\n",
    "class BasicNet(nn.Module):\n",
    "    \n",
    "    def __init__(self, Ni, Nh1, Nh2, No):\n",
    "        \"\"\"\n",
    "        Ni - Input size\n",
    "        Nh1 - Neurons in the 1st hidden layer\n",
    "        Nh2 - Neurons in the 2nd hidden layer\n",
    "        No - Output size\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(in_features=Ni, out_features=Nh1)\n",
    "        self.fc2 = nn.Linear(in_features=Nh1, out_features=Nh2)\n",
    "        self.out = nn.Linear(in_features=Nh2, out_features=No)\n",
    "        self.soft = nn.Softmax()\n",
    "        \n",
    "        self.relu= nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.name=\"BasicNet\"\n",
    "\n",
    "        print('Network initialized')\n",
    "        \n",
    "    def forward(self, input, additional_out=False):\n",
    "        x=input\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.out(x)\n",
    "        x= self.soft(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2) Convolutional Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CONVOLUTIONAL NN\n",
    "import torch.nn.functional as F\n",
    "class Cnn(nn.Module):\n",
    "    def __init__(self, dropout=0.5,conv1=32,conv2=64,fc1=128):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(1, conv1, kernel_size=3)\n",
    "        self.conv2 = nn.Conv2d(conv1, conv2, kernel_size=3)\n",
    "        self.conv2_drop = nn.Dropout2d(p=dropout)\n",
    "        self.fc1 = nn.Linear(1600, fc1) # 1600 = number channels * width * height\n",
    "        self.fc2 = nn.Linear(fc1, 10)\n",
    "        self.fc1_drop = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = torch.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        \n",
    "        # flatten over channel, height and width = 1600\n",
    "        x = x.view(-1, x.size(1) * x.size(2) * x.size(3))\n",
    "        \n",
    "        x = torch.relu(self.fc1_drop(self.fc1(x)))\n",
    "        x = torch.softmax(self.fc2(x), dim=-1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1) Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Check if the GPU is available\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(f\"Training device: {device}\")\n",
    "\n",
    "# Define the loss function\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "#Early Stopping\n",
    "from skorch.callbacks import EarlyStopping\n",
    "\n",
    "my_early = EarlyStopping(\n",
    "    monitor='valid_loss',\n",
    "    patience=10,\n",
    "    threshold=0.0001,\n",
    "    threshold_mode='rel',\n",
    "    lower_is_better=True)\n",
    "\n",
    "#Model initialization\n",
    "from skorch import NeuralNetClassifier\n",
    "\n",
    "#FULLY CONNECTED\n",
    "net = NeuralNetClassifier(\n",
    "    module=BasicNet,\n",
    "    module__Ni= 784,\n",
    "    module__Nh1 = 8,\n",
    "    module__Nh2 = 48,\n",
    "    module__No = 10,\n",
    "    max_epochs=50,\n",
    "    \n",
    "    device=device,  # uncomment this to train with CUDA\n",
    "    #lr=0.1,\n",
    "    #optimizer = optim.SGD,\n",
    "    optimizer = optim.Adam,\n",
    "    optimizer__lr=0.001,\n",
    "    optimizer__weight_decay=1e-5, #L2 norm\n",
    "    criterion=nn.CrossEntropyLoss,\n",
    "    callbacks = [my_early],\n",
    "    #verbose=0\n",
    ")\n",
    "\n",
    "#CNN\n",
    "net = NeuralNetClassifier(\n",
    "    module=Cnn,\n",
    "    module__conv1=32,\n",
    "    module__conv2=32,\n",
    "    module__fc1=32,\n",
    "    max_epochs=50,\n",
    "    #lr=0.002,\n",
    "    device=device,\n",
    "    optimizer = optim.Adam,\n",
    "    optimizer__lr=0.002,\n",
    "    optimizer__weight_decay=1e-5, #L2 norm\n",
    "    criterion=nn.CrossEntropyLoss,\n",
    "    callbacks = [my_early],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2) Grid Search and Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "#FULLY CONNECTED\n",
    "params = {\n",
    "    'module__Nh1': [8,16,32,48],\n",
    "    'module__Nh2': [8,16,32,48],\n",
    "    'max_epochs': [1500],\n",
    "    'optimizer__lr':[0.1, 0.01, 0.001],\n",
    "    'optimizer__weight_decay':[1e-3,1e-4,1e-5] #L2 norm,\n",
    "}\n",
    "params = {\n",
    "    'module__Nh1': [8,48],\n",
    "    'module__Nh2': [8,48],\n",
    "    'max_epochs': [1500],\n",
    "    'optimizer__lr':[0.01, 0.001],\n",
    "    'optimizer__weight_decay':[1e-4,1e-5] #L2 norm,\n",
    "}\n",
    "\n",
    "#CNN\n",
    "params = {\n",
    "    'module__Nh1': [8,48],\n",
    "    'module__Nh2': [8,48],\n",
    "    'max_epochs': [1500],\n",
    "    'optimizer__lr':[0.01, 0.001],\n",
    "    'optimizer__weight_decay':[1e-4,1e-5] #L2 norm,\n",
    "}\n",
    "\n",
    "\n",
    "gs = GridSearchCV(net, params, refit=True, cv=3, scoring=\"neg_mean_squared_error\",verbose=10)\n",
    "\n",
    "gs.fit(X_train, y_train)\n",
    "\n",
    "print(gs.best_score_, gs.best_params_)\n",
    "net=gs.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3) Normal Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m1.7627\u001b[0m       \u001b[32m0.9332\u001b[0m        \u001b[35m1.5304\u001b[0m  3.7377\n",
      "      2        \u001b[36m1.5418\u001b[0m       \u001b[32m0.9645\u001b[0m        \u001b[35m1.4967\u001b[0m  3.5663\n",
      "      3        \u001b[36m1.5233\u001b[0m       \u001b[32m0.9687\u001b[0m        \u001b[35m1.4928\u001b[0m  3.6830\n",
      "      4        \u001b[36m1.5160\u001b[0m       \u001b[32m0.9717\u001b[0m        \u001b[35m1.4893\u001b[0m  3.5606\n",
      "      5        \u001b[36m1.5092\u001b[0m       \u001b[32m0.9772\u001b[0m        \u001b[35m1.4841\u001b[0m  3.6598\n",
      "      6        \u001b[36m1.5066\u001b[0m       \u001b[32m0.9788\u001b[0m        \u001b[35m1.4822\u001b[0m  3.5958\n",
      "      7        \u001b[36m1.5018\u001b[0m       0.9781        1.4827  3.4942\n",
      "      8        \u001b[36m1.4997\u001b[0m       \u001b[32m0.9791\u001b[0m        \u001b[35m1.4817\u001b[0m  3.4795\n",
      "      9        \u001b[36m1.4990\u001b[0m       \u001b[32m0.9798\u001b[0m        \u001b[35m1.4811\u001b[0m  3.5110\n",
      "     10        \u001b[36m1.4972\u001b[0m       \u001b[32m0.9816\u001b[0m        \u001b[35m1.4797\u001b[0m  3.5265\n",
      "     11        \u001b[36m1.4943\u001b[0m       \u001b[32m0.9818\u001b[0m        \u001b[35m1.4794\u001b[0m  3.4556\n",
      "     12        1.4961       \u001b[32m0.9822\u001b[0m        \u001b[35m1.4791\u001b[0m  3.5175\n",
      "     13        1.4954       0.9815        1.4794  3.4946\n",
      "     14        \u001b[36m1.4936\u001b[0m       0.9797        1.4812  3.4721\n",
      "     15        \u001b[36m1.4935\u001b[0m       \u001b[32m0.9836\u001b[0m        \u001b[35m1.4773\u001b[0m  3.5300\n",
      "     16        \u001b[36m1.4919\u001b[0m       0.9832        1.4779  3.4761\n",
      "     17        1.4927       0.9830        1.4782  3.4898\n",
      "     18        1.4924       \u001b[32m0.9848\u001b[0m        \u001b[35m1.4763\u001b[0m  3.5417\n",
      "     19        \u001b[36m1.4912\u001b[0m       0.9826        1.4787  3.4801\n",
      "     20        1.4913       0.9836        1.4774  3.5143\n",
      "     21        1.4917       0.9847        1.4765  3.5575\n",
      "     22        \u001b[36m1.4897\u001b[0m       0.9833        1.4777  3.6566\n",
      "     23        1.4900       \u001b[32m0.9851\u001b[0m        1.4763  3.6333\n",
      "     24        \u001b[36m1.4891\u001b[0m       \u001b[32m0.9856\u001b[0m        \u001b[35m1.4754\u001b[0m  3.6926\n",
      "     25        1.4902       \u001b[32m0.9869\u001b[0m        \u001b[35m1.4743\u001b[0m  3.6418\n",
      "     26        1.4907       0.9853        1.4757  3.5419\n",
      "     27        1.4895       0.9863        1.4750  3.5909\n",
      "     28        1.4909       0.9837        1.4774  3.7179\n",
      "     29        \u001b[36m1.4883\u001b[0m       0.9845        1.4768  3.8663\n",
      "     30        1.4893       0.9860        1.4751  3.8926\n",
      "     31        1.4889       0.9866        \u001b[35m1.4743\u001b[0m  3.7787\n",
      "     32        1.4895       0.9868        1.4745  3.7885\n",
      "     33        1.4894       0.9851        1.4761  3.6906\n",
      "     34        \u001b[36m1.4878\u001b[0m       0.9867        1.4744  3.6727\n",
      "Stopping since valid_loss has not improved in the last 10 epochs.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<class 'skorch.classifier.NeuralNetClassifier'>[initialized](\n",
       "  module_=Cnn(\n",
       "    (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
       "    (fc1): Linear(in_features=1600, out_features=128, bias=True)\n",
       "    (fc2): Linear(in_features=128, out_features=10, bias=True)\n",
       "    (fc1_drop): Dropout(p=0.5, inplace=False)\n",
       "  ),\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4) Plot Losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "save_name=\"Classification_\"+datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M\")\n",
    "\n",
    "# get train losses from all epochs, a list of floats\n",
    "history = net.history\n",
    "train_loss_log=history[:, 'train_loss']\n",
    "val_loss_log=history[:, 'valid_loss']\n",
    "\n",
    "# Plot losses\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.plot(train_loss_log)\n",
    "plt.plot(val_loss_log)\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['Train loss', 'Validation loss'], loc='upper right')\n",
    "plt.savefig(\"models/\"+save_name+\"_Losses\", dpi=400)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "y_pred = net.predict(X_test)\n",
    "\n",
    "test_acc = accuracy_score(y_test, y_pred)\n",
    "print(test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) Metrics Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues,\n",
    "\t\t\t\t\t\t  save_path='models/'):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        #print(\"Normalized confusion matrix\")\n",
    "    #else:\n",
    "        #print('Confusion matrix, without normalization')\n",
    "\n",
    "    plt.figure(figsize=(15, 15))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title, fontsize=30)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45, fontsize=15)\n",
    "    plt.yticks(tick_marks, classes, fontsize=15)\n",
    "\n",
    "    fmt = '.3f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt), size=11,\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.ylabel('True label', fontsize=30)\n",
    "    plt.xlabel('Predicted label', fontsize=30)\n",
    "    plt.savefig(save_path+\"_picConfMatrix.png\", dpi=400)\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Accuracy\n",
    "val_acc=history[:, 'valid_acc'][-1]\n",
    "val_loss=history[:, 'valid_loss'][-1]\n",
    "train_loss=history[:, 'train_loss'][-1]\n",
    "\n",
    "print(\"Val Acc:\\t\",round(val_acc,3))\n",
    "print(\"Test Acc:\\t\",round(float(test_acc),3))\n",
    "\n",
    "# Precision and Recall(sensitivity/true positive rate)\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "prec=precision_score(y_test, y_pred,average='micro')\n",
    "rec=recall_score(y_test, y_pred,average='micro')\n",
    "\n",
    "#F1 - high if both recall and precision are high.\n",
    "from sklearn.metrics import f1_score\n",
    "f1=f1_score(y_test, y_pred,average='micro')\n",
    "\n",
    "print(\"Precision:\\t\",round(prec,3))\n",
    "print(\"Recall:\\t\\t\",round(rec,3))\n",
    "print(\"F1:\\t\\t\",round(f1,3))\n",
    "\n",
    "# Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "categories=[0,1,2,3,4,5,6,7,8,9]\n",
    "plot_confusion_matrix(cm,categories, normalize=False,save_path=\"models/\"+save_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6) Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving the whole model\n",
    "import pickle\n",
    "with open(\"models/\"+save_name+\".pkl\", 'wb') as f:\n",
    "    pickle.dump(net, f)\n",
    "    \n",
    "#Load the model\n",
    "#with open(file_name, 'rb') as f:\n",
    "#    new_net = pickle.load(f)\n",
    "\n",
    "#Save Metrics to File\n",
    "f = open(\"models/\"+save_name+\"_Metrics.txt\", \"a\")\n",
    "f.write('Train loss:\\t'+ str(round(train_loss,3))+ \"\\n\")\n",
    "f.write('Val loss:\\t'+ str(round(val_loss,3))+ \"\\n\")\n",
    "f.write('Val acc:\\t'+ str(round(val_acc,3))+ \"\\n\")\n",
    "f.write('Test acc:\\t'+ str(round(test_acc,3))+ \"\\n\")\n",
    "f.write(\"Precision:\\t\"+str(round(prec,3))+ \"\\n\")\n",
    "f.write(\"Recall:\\t\\t\"+str(round(rec,3))+ \"\\n\")\n",
    "f.write(\"F1:\\t\\t\"+str(round(f1,3)))\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Most mispredicted labels\n",
    "n_mistakes=10\n",
    "import heapq\n",
    "h=[]\n",
    "nCategories=10\n",
    "for i in range(nCategories):\n",
    "    for j in range(i+1,nCategories):\n",
    "        heapq.heappush(h,(cm[i,j]+cm[j,i],(i,j)))\n",
    "for e in heapq.nlargest(n_mistakes,h):\n",
    "    print(e[0],e[1][0],\"-\",e[1][1])\n",
    "\n",
    "f = open(\"models/\"+save_name+\"_Metrics.txt\", \"a\")\n",
    "f.write('\\n\\nMost '+str(n_mistakes)+ ' mispredicted labels\\n')\n",
    "for e in heapq.nlargest(n_mistakes,h):\n",
    "    f.write(str(e[0])+\"\\t\"+str(categories[e[1][0]])+\"-\"+str(categories[e[1][1]])+\"\\n\")\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7) Network Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1) Weights histogram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.1.1) 3FC Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Access network parameters\n",
    "my_best_net = net.module_\n",
    "\n",
    "#First hidden Layer\n",
    "h1_w = my_best_net.fc1.weight.data.cpu().numpy()\n",
    "h1_b = my_best_net.fc1.bias.data.cpu().numpy()\n",
    "\n",
    "#Second hidden Layer\n",
    "h2_w = my_best_net.fc2.weight.data.cpu().numpy()\n",
    "h2_b = my_best_net.fc2.bias.data.cpu().numpy()\n",
    "\n",
    "# Output layer\n",
    "out_w = my_best_net.out.weight.data.cpu().numpy()\n",
    "out_b = my_best_net.out.bias.data.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weights histogram\n",
    "fig, axs = plt.subplots(3, 1, figsize=(12,8))\n",
    "axs[0].hist(h1_w.flatten(), 50)\n",
    "axs[0].set_title('First hidden layer weights')\n",
    "axs[1].hist(h2_w.flatten(), 50)\n",
    "axs[1].set_title('Second hidden layer weights')\n",
    "axs[2].hist(out_w.flatten(), 50)\n",
    "axs[2].set_title('Output layer weights')\n",
    "[ax.grid() for ax in axs]\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"models/\"+save_name+\"_Weights-histogram\", dpi=400)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.1.2) Convolution network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Access network parameters\n",
    "my_best_net = net.module_\n",
    "\n",
    "#First hidden Layer\n",
    "h1_w = my_best_net.fc1.weight.data.cpu().numpy()\n",
    "h1_b = my_best_net.fc1.bias.data.cpu().numpy()\n",
    "\n",
    "#Second hidden Layer\n",
    "h2_w = my_best_net.fc2.weight.data.cpu().numpy()\n",
    "h2_b = my_best_net.fc2.bias.data.cpu().numpy()\n",
    "\n",
    "# Conv 1 layer\n",
    "c1_w = my_best_net.conv1.weight.data.cpu().numpy()\n",
    "c1_b = my_best_net.conv1.bias.data.cpu().numpy()\n",
    "\n",
    "# Conv 2 layer\n",
    "c2_w = my_best_net.conv2.weight.data.cpu().numpy()\n",
    "c2_b = my_best_net.conv2.bias.data.cpu().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weights histogram\n",
    "fig, axs = plt.subplots(4, 1, figsize=(12,8))\n",
    "axs[0].hist(c1_w.flatten(), 50)\n",
    "axs[0].set_title('Conv1 layer weights')\n",
    "axs[1].hist(c2_w.flatten(), 50)\n",
    "axs[1].set_title('Conv2 layer weights')\n",
    "axs[2].hist(h1_w.flatten(), 50)\n",
    "axs[2].set_title('First FC layer weights')\n",
    "axs[3].hist(h2_w.flatten(), 50)\n",
    "axs[3].set_title('Second FC layer weights')\n",
    "\n",
    "[ax.grid() for ax in axs]\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"models/\"+save_name+\"_Weights-histogram\", dpi=400)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2) Analyze activations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.2.1) 3FC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "??????????????????\n",
    "ù\"\"\"def get_activation(layer, input, output):\n",
    "    global activation\n",
    "    activation = torch.softmax(output)\n",
    "    \n",
    "### Register hook\n",
    "net=my_best_net\n",
    "hook_handle = net.fc2.register_forward_hook(get_activation)\n",
    "\n",
    "\n",
    "### Analyze activations\n",
    "net = net.to(device)\n",
    "net.eval()\n",
    "with torch.no_grad():\n",
    "    x1 = torch.from_numpy(X_test[0]).to(device)\n",
    "    y1 = net(x1)\n",
    "    z1 = activation\n",
    "    x2 = torch.tensor(X_test[1]).float().to(device)\n",
    "    y2 = net(x2)\n",
    "    z2 = activation\n",
    "    x3 = torch.tensor(X_test[2]).float().to(device)\n",
    "    y3 = net(x3)\n",
    "    z3 = activation\n",
    "\n",
    "### Remove hook\n",
    "hook_handle.remove()\n",
    "\n",
    "### Plot activations\n",
    "fig, axs = plt.subplots(3, 1, figsize=(12,6))\n",
    "axs[0].stem(z1.cpu().numpy(), use_line_collection=True)\n",
    "axs[0].set_title('Last layer activations for input x=%.2f' % x1)\n",
    "axs[1].stem(z2.cpu().numpy(), use_line_collection=True)\n",
    "axs[1].set_title('Last layer activations for input x=%.2f' % x2)\n",
    "axs[2].stem(z3.cpu().numpy(), use_line_collection=True)\n",
    "axs[2].set_title('Last layer activations for input x=%.2f' % x3)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"models/\"+save_name+\"_Activations\", dpi=400)\n",
    "plt.show()\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.2.2) Convolution Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#......"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3) Receptive fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "linear combination method for visualising the features discussed in class, which is straightforward for the fully-connected model. Then if you wish you can explore more sophisticated methods, such as the method that allows to create an \"optimal\" image that maximally activates the neuron. This can be \"easily\" done also for the CNN. In any case, receptive fields are meaningful only for the classification task.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.3.1) 3FC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FULLY CONNECTED NET\n",
    "#Receptive fields of the last layer\n",
    "for ii in range(10):\n",
    "    vis3 = np.matmul(out_w[ii],np.matmul(h2_w,h1_w))\n",
    "    print(\"\\nLabel:\", ii)\n",
    "    plt.imshow(vis3.reshape(784).reshape(28,28))\n",
    "    plt.savefig(\"models/\"+save_name+\"_Receptive fields_\"+str(ii), dpi=400)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.3.2) Convolution Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#....."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gputest",
   "language": "python",
   "name": "gputest"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
