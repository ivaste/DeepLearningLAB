{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "nndl_2020__homework_3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zYncZVOyF8Rb"
      },
      "source": [
        "#NEURAL NETWORKS AND DEEP LEARNING\n",
        "> M.Sc. ICT FOR LIFE AND HEALTH\n",
        "> \n",
        "> Department of Information Engineering\n",
        "\n",
        "> M.Sc. COMPUTER ENGINEERING\n",
        ">\n",
        "> Department of Information Engineering\n",
        "\n",
        "> M.Sc. AUTOMATION ENGINEERING\n",
        ">\n",
        "> Department of Information Engineering\n",
        " \n",
        "> M.Sc. PHYSICS OF DATA\n",
        ">\n",
        "> Department of Physics and Astronomy\n",
        " \n",
        "> M.Sc. COGNITIVE NEUROSCIENCE AND CLINICAL NEUROPSYCHOLOGY\n",
        ">\n",
        "> Department of General Psychology\n",
        "\n",
        "---\n",
        "A.A. 2020/21 (6 CFU) - Dr. Alberto Testolin, Dr. Matteo Gadaleta\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AgHJFDwYvgHg"
      },
      "source": [
        "# Homework 3 - Deep Reinforcement Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bNENm7RHGFMd"
      },
      "source": [
        "## General overview\n",
        "In this homework you will learn how to implement and test neural network models for solving reinforcement learning problems. The basic tasks for the homework will require to implement some extensions to the code that you have seen in the Lab. More advanced tasks will require to train and test your learning agent on a different environment. Given the higher computational complexity of RL, in this homework you don’t need to tune learning hyperparameters using search procedures and cross-validation; however, you are encouraged to play with model hyperparameters in order to find a satisfactory configuration."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bZGjMokMvk2h"
      },
      "source": [
        "\n",
        "## Technical notes\n",
        "The homework should be implemented in Python using the PyTorch framework. The student can explore additional libraries and tools to implement the models; however, please make sure you understand the code you are writing because during the exam you might receive specific questions related to your implementation. The entire source code required to run the homework must be uploaded as a compressed archive in a Moodle section dedicated to the homework. If your code will be entirely included in a single Python notebook, just upload the notebook file.\n",
        "\n",
        "As an example of more advanced libraries that can be used to implement deep RL agents, you can check this website:\n",
        "\n",
        "https://stable-baselines.readthedocs.io/en/master/\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UYZUBEMCvlpB"
      },
      "source": [
        "\n",
        "## Final report\n",
        "Along with the source code, you must separately upload a PDF file containing a brief report of your homework. The report should include a brief Introduction on which you explain the homework goals and the main implementation strategies you choose, a brief Method section where you describe your model architectures and hyperparameters, and a Result section where you present the simulation results. Total length must not exceed 6 pages, though you can include additional tables and figures in a final Appendix (optional). Given the dynamical nature of RL problems, you can explore more sophisticated media for showing the results of your model (e.g., animated GIFs or short movies).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RchPc7G6vmRB"
      },
      "source": [
        "\n",
        "## Grade\n",
        "The maximum grade for this homework will be **8 points**. Points will be assigned based on the correct implementation of the following items:\n",
        "*\t2 pt: extend the notebook used in Lab 07, in order to study how the exploration profile (either using eps-greedy or softmax) impacts the learning curve. Try to tune the model hyperparameters or tweak the reward function in order to speed-up learning convergence (i.e., reach the same accuracy with fewer training episodes).\n",
        "*\t3 pt: extend the notebook used in Lab 07, in order to learn to control the CartPole environment using directly the screen pixels, rather than the compact state representation used during the Lab (cart position, cart velocity, pole angle, pole angular velocity). This will require to change the “observation_space”.\n",
        "*\t3 pt: train a deep RL agent on a different Gym environment. You are free to choose whatever Gym environment you like from the available list, or even explore other simulation platforms:\n",
        "https://gym.openai.com/envs \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LYE6Cjhgvm3B"
      },
      "source": [
        "\n",
        "## Deadline\n",
        "The complete homework (source code + report) must be submitted through Moodle at least 10 days before the chosen exam date."
      ]
    }
  ]
}